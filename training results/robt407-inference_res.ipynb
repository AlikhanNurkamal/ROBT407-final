{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5048,"databundleVersionId":868335,"sourceType":"competition"},{"sourceId":7106292,"sourceType":"datasetVersion","datasetId":4096895}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision import transforms\n\ntorch.manual_seed(42)\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:02:52.298263Z","iopub.execute_input":"2023-12-03T11:02:52.299154Z","iopub.status.idle":"2023-12-03T11:02:56.276634Z","shell.execute_reply.started":"2023-12-03T11:02:52.299119Z","shell.execute_reply":"2023-12-03T11:02:56.275871Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Setting up config Parameters","metadata":{}},{"cell_type":"code","source":"config = {\n    'ROOT_DIR': '/kaggle/input/state-farm-distracted-driver-detection/imgs/train',\n    'TEST_DIR': '/kaggle/input/state-farm-distracted-driver-detection/imgs/test',\n    'MODELS_DIR': '/kaggle/working/',\n    'IMG_SIZE': 224,\n    'BATCH_SIZE': 250,\n    'EPOCHS': 20,\n    'PATIENCE': 5,\n    'LR_INIT': 1e-4,\n    'WEIGHT_DECAY': 5e-3,\n    'NUM_CLASSES': 10,\n    'NUM_WORKERS': 4,\n    'NUM_CHANNELS': 3,\n    'DEVICE': torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:02:58.072238Z","iopub.execute_input":"2023-12-03T11:02:58.072735Z","iopub.status.idle":"2023-12-03T11:02:58.100497Z","shell.execute_reply.started":"2023-12-03T11:02:58.072707Z","shell.execute_reply":"2023-12-03T11:02:58.099536Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Implementation of models","metadata":{}},{"cell_type":"code","source":"# Block for ResNet50 and ResNet101 architectures\nclass Block(nn.Module):\n    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n        \"\"\"\n        :param in_channels: number of input channels\n        :param out_channels: number of output channels\n        :param identity_downsample: Conv layer to downsample image in case of different input and output channels\n        :param stride: stride\n        \"\"\"\n        super().__init__()\n        self.identity_downsample = identity_downsample\n\n        # every block in ResNet50 or deeper increases the number of in_channels by 4\n        self.expansion = 4\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        )\n        \n        self.conv3 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0),\n            nn.BatchNorm2d(out_channels * self.expansion),\n            nn.ReLU()\n        )\n    \n    def forward(self, x):\n        identity = x\n\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n\n        # if the input and output channels are different, then downsample (with no activation, hence identity) the input image\n        if self.identity_downsample:\n            identity = self.identity_downsample(identity)\n        \n        # add the identity (input image) to the output of the block\n        x = x + identity\n        x = F.relu(x)\n        return x\n\n# ResNet50 and ResNet101 architectures\nclass MyResNet(nn.Module):\n    def __init__(self, num_layers, in_channels, out_classes):\n        \"\"\"\n        :param num_layers: number of layers in the architecture (ResNet)\n        :param in_channels: number of input image channels\n        :param out_classes: number of output classes\n        \"\"\"\n        assert num_layers in [50, 101], 'unknown architecture'\n\n        super().__init__()\n\n        # how many times to reuse the same block in the architecture\n        if num_layers == 50:\n            layers = [3, 4, 6, 3]\n        elif num_layers == 101:\n            layers = [3, 4, 23, 3]\n        else:\n            raise NotImplementedError('unknown architecture')\n        \n        self.in_channels = 64\n\n        # according to the paper, the first layer is 7x7 conv with stride 2 and padding 3\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n        )\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # ResNet layers\n        self.layer1 = self._make_layer(layers[0], 64, stride=1)\n        self.layer2 = self._make_layer(layers[1], 128, stride=2)\n        self.layer3 = self._make_layer(layers[2], 256, stride=2)\n        self.layer4 = self._make_layer(layers[3], 512, stride=2)\n\n        # according to the paper, the last layer is avgpool with output size 1x1\n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n        self.fc = nn.Linear(512 * 4, out_classes)\n    \n    def _make_layer(self, num_residual_blocks, in_channels, stride):\n        \"\"\"\n        :param num_residual_blocks: how many times to reuse the same block in the architecture\n        :param in_channels: number of input channels, output channels are 4 times larger\n        :param stride: stride\n        :return: layers of residual blocks\n        \"\"\"\n        identity_downsample = None\n        layers = []\n\n        if stride != 1 or self.in_channels != in_channels * 4:\n            identity_downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, in_channels * 4, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(in_channels * 4)\n            )\n        \n        # perform the first residual block\n        layers.append(Block(self.in_channels, in_channels, identity_downsample, stride))\n        self.in_channels = in_channels * 4\n\n        # perform the rest of the residual blocks\n        for i in range(num_residual_blocks - 1):\n            layers.append(Block(self.in_channels, in_channels))\n        \n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        # first 7x7 conv layer\n        x = self.conv1(x)\n        x = self.maxpool(x)\n\n        # ResNet layers\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        # last avgpool layer plus fully connected layer\n        x = self.avgpool(x)\n        x = x.view(x.shape[0], -1)\n        x = self.fc(x)\n\n        return x\n\n\n# in this project we have 10 classes to predict, so out_classes=10\ndef ResNet50(in_channels=3, out_classes=10):\n    return MyResNet(50, in_channels, out_classes=out_classes)\n\n\n# in this project we have 10 classes to predict, so out_classes=10\ndef ResNet101(in_channels=3, out_classes=10):\n    return MyResNet(101, in_channels, out_classes=out_classes)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:02:59.673880Z","iopub.execute_input":"2023-12-03T11:02:59.674299Z","iopub.status.idle":"2023-12-03T11:02:59.696589Z","shell.execute_reply.started":"2023-12-03T11:02:59.674261Z","shell.execute_reply":"2023-12-03T11:02:59.695601Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# MultiHeadAttention Module\nclass MSA(nn.Module):\n    def __init__(self, embedding_dim: int=192, num_heads: int=3, dropout: float=0):\n        super().__init__()\n        # layer normalization layer\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n        # multiheadattention layer\n        self.msa_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n                                                   num_heads=num_heads,\n                                                   dropout=dropout,\n                                                   batch_first=True)\n        \n    def forward(self, x):\n        x = self.layer_norm(x)\n        # query, key, value are formed from the same x\n        x, _ = self.msa_attention(query=x,\n                                    key=x,\n                                    value=x,\n                                    need_weights=False)\n        return x\n    \n# MLP layer   \nclass MLP(nn.Module):\n    def __init__(self, \n                 embedding_dim: int=192,\n                 mlp_size: int=768,\n                 dropout: float=0.1):\n        super().__init__()\n        # layer normalization\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n        # two linear layers separated by GeLU activation\n        self.mlp = nn.Sequential(\n                nn.Linear(in_features=embedding_dim,\n                        out_features=mlp_size), # from input_dim to mlp_size\n                nn.GELU(),\n                nn.Dropout(p=dropout),\n                nn.Linear(in_features=mlp_size, # from mlp_size to input_dim\n                        out_features=embedding_dim),\n                nn.Dropout(p=dropout))\n        \n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n        \n        return x\n        \n# Transformer encoder block\nclass EncoderBlock(nn.Module):\n    def __init__(self,\n                 embedding_dim: int=192,\n                 num_heads: int=3,\n                 mlp_size: int=768,\n                 mlp_dropout: float=0.1,\n                 msa_dropout: float=0.0):\n        super().__init__()\n        # each block (layer) consists of MSA and MLP\n        self.msa = MSA(embedding_dim=embedding_dim,\n                       num_heads=num_heads,\n                       dropout=msa_dropout)\n        \n        self.mlp = MLP(embedding_dim=embedding_dim,\n                       mlp_size=mlp_size,\n                       dropout=mlp_dropout)\n        \n    def forward(self, x):\n        x = self.msa(x) + x # skip connection #1\n        x = self.mlp(x) + x # skip connection #2\n        \n        return x\n\n\n# taken from CVT-CCT paper\nclass Tokenizer(nn.Module):\n    def __init__(self,\n                 kernel_size: int=7, stride: int=2, padding: int=3,\n                 pooling_kernel_size: int=3, pooling_stride: int=2, pooling_padding: int=1,\n                 conv_layers: int=2, input_channels: int=3, \n                 output_channels: int=64, in_planes: int=64):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(input_channels, in_planes,\n                               kernel_size=(kernel_size, kernel_size),\n                               stride=(stride, stride),\n                               padding=(padding, padding), bias=False)\n        \n        self.max_pool = nn.MaxPool2d(kernel_size=pooling_kernel_size,\n                                     stride=pooling_stride,\n                                     padding=pooling_padding)\n        \n        self.conv2 = nn.Conv2d(in_planes, output_channels,\n                               kernel_size=(kernel_size, kernel_size),\n                               stride=(stride, stride),\n                               padding=(padding, padding), bias=False)\n\n        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.max_pool(x)\n        x = self.conv2(x)\n        x = self.max_pool(x)\n        x = self.flatten(x)\n        \n        return x.permute(0, 2, 1)\n\n\nclass CvT(nn.Module):\n    def __init__(self,\n                 img_size: int=224, in_channels: int=3, kernel_size: int=7,\n                 layers: int=7, embedding_dim: int=256, mlp_size: int=512,\n                 num_heads: int=4, stride: int=2, padding: int=3,\n                 pooling_kernel_size: int=3, pooling_stride: int=2, pooling_padding :int=1,\n                 conv_layers: int=2, in_planes: int=64,\n                 msa_dropout: float=0.0, mlp_dropout: float=0.1,\n                 emb_dropout: float=0.1, num_classes: int=10):\n        super().__init__()\n        \n        self.tokenizer = Tokenizer(kernel_size=kernel_size, stride=stride, \n                                   padding=padding, pooling_kernel_size=pooling_kernel_size, \n                                   pooling_stride=pooling_stride, pooling_padding=pooling_padding, \n                                   conv_layers=conv_layers, input_channels=in_channels, \n                                   output_channels=embedding_dim, in_planes=in_planes)\n        \n        self.attention_pool = nn.Linear(embedding_dim, 1)\n        \n        self.emb_dropout = nn.Dropout(p=emb_dropout)\n        \n        self.encoder = nn.Sequential(*[\n            EncoderBlock(embedding_dim=embedding_dim,\n                         num_heads=num_heads,\n                         mlp_size=mlp_size,\n                         mlp_dropout=mlp_dropout,\n                         msa_dropout=msa_dropout)\n            for _ in range(layers)\n        ])\n        \n        self.norm = nn.LayerNorm(embedding_dim)\n        \n        self.head = nn.Sequential(\n            nn.LayerNorm(normalized_shape=embedding_dim),\n            nn.Linear(in_features=embedding_dim,\n                      out_features=num_classes)\n        )\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        \n        x = self.tokenizer(x)\n        x = self.emb_dropout(x)\n        x = self.encoder(x)\n        x = self.norm(x)\n        x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n        x = self.head(x)\n        \n        return x\n    \nclass CvT_3(CvT):\n    def __init__(self,\n                 img_size: int=224, in_channels: int=3, kernel_size: int=7,\n                 layers: int=3, embedding_dim: int=192, mlp_size: int=384,\n                 num_heads: int=3, stride: int=2, padding: int=3,\n                 pooling_kernel_size: int=3, pooling_stride: int=2, pooling_padding :int=1,\n                 conv_layers: int=2, in_planes: int=64,\n                 msa_dropout: float=0.0, mlp_dropout: float=0.1,\n                 emb_dropout: float=0.1, num_classes: int=10):\n        super().__init__(layers=layers, embedding_dim=embedding_dim, \n                         mlp_size=mlp_size, num_heads=num_heads)\n    \nclass CvT_7(CvT):\n    def __init__(self,\n                 img_size: int=224, in_channels: int=3, kernel_size: int=7,\n                 layers: int=7, embedding_dim: int=256, mlp_size: int=512,\n                 num_heads: int=4, stride: int=2, padding: int=3,\n                 pooling_kernel_size: int=3, pooling_stride: int=2, pooling_padding :int=1,\n                 conv_layers: int=2, in_planes: int=64,\n                 msa_dropout: float=0.0, mlp_dropout: float=0.1,\n                 emb_dropout: float=0.1, num_classes: int=10):\n        super().__init__(layers=layers, embedding_dim=embedding_dim, \n                         mlp_size=mlp_size, num_heads=num_heads)\n    \nclass CvT_14(CvT):\n    def __init__(self,\n                 img_size: int=224, in_channels: int=3, kernel_size: int=7,\n                 layers: int=14, embedding_dim: int=384, mlp_size: int=1152,\n                 num_heads: int=6, stride: int=2, padding: int=3,\n                 pooling_kernel_size: int=3, pooling_stride: int=2, pooling_padding :int=1,\n                 conv_layers: int=2, in_planes: int=64,\n                 msa_dropout: float=0.0, mlp_dropout: float=0.1,\n                 emb_dropout: float=0.1, num_classes: int=10):\n        super().__init__(layers=layers, embedding_dim=embedding_dim, \n                         mlp_size=mlp_size, num_heads=num_heads)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:17:36.842440Z","iopub.execute_input":"2023-12-03T11:17:36.842872Z","iopub.status.idle":"2023-12-03T11:17:36.880688Z","shell.execute_reply.started":"2023-12-03T11:17:36.842841Z","shell.execute_reply":"2023-12-03T11:17:36.879664Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# convering image into embeddings\nclass PatchEmbedding(nn.Module):\n    def __init__(self, in_channels: int=3, patch_size: int=16, embedding_dim: int=192):\n        super().__init__()\n        # patches are extracted using convolution, \n        self.patches = nn.Conv2d(in_channels=in_channels,\n                                 out_channels=embedding_dim,\n                                 kernel_size=patch_size,\n                                 stride=patch_size, # non overlapping\n                                 padding=0)\n        # extracted patches are then flattened into embeddings\n        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n        \n        \n    def forward(self, x):\n        x = self.patches(x)\n        x = self.flatten(x)\n        \n        return x.permute(0, 2, 1)\n        \n        \n# MultiHeadAttention Module\nclass MSA(nn.Module):\n    def __init__(self, embedding_dim: int=192, num_heads: int=3, dropout: float=0):\n        super().__init__()\n        # layer normalization layer\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n        # multiheadattention layer\n        self.msa_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n                                                   num_heads=num_heads,\n                                                   dropout=dropout,\n                                                   batch_first=True)\n        \n    def forward(self, x):\n        x = self.layer_norm(x)\n        # query, key, value are formed from the same x\n        x, _ = self.msa_attention(query=x,\n                                    key=x,\n                                    value=x,\n                                    need_weights=False)\n        return x\n    \n# MLP layer   \nclass MLP(nn.Module):\n    def __init__(self, \n                 embedding_dim: int=192,\n                 mlp_size: int=768,\n                 dropout: float=0.1):\n        super().__init__()\n        # layer normalization\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n        # two linear layers separated by GeLU activation\n        self.mlp = nn.Sequential(\n                nn.Linear(in_features=embedding_dim,\n                        out_features=mlp_size), # from input_dim to mlp_size\n                nn.GELU(),\n                nn.Dropout(p=dropout),\n                nn.Linear(in_features=mlp_size, # from mlp_size to input_dim\n                        out_features=embedding_dim),\n                nn.Dropout(p=dropout))\n        \n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n        \n        return x\n        \n# Transformer encoder block\nclass EncoderBlock(nn.Module):\n    def __init__(self,\n                 embedding_dim: int=192,\n                 num_heads: int=3,\n                 mlp_size: int=768,\n                 mlp_dropout: float=0.1,\n                 msa_dropout: float=0.0):\n        super().__init__()\n        # each block (layer) consists of MSA and MLP\n        self.msa = MSA(embedding_dim=embedding_dim,\n                       num_heads=num_heads,\n                       dropout=msa_dropout)\n        \n        self.mlp = MLP(embedding_dim=embedding_dim,\n                       mlp_size=mlp_size,\n                       dropout=mlp_dropout)\n        \n    def forward(self, x):\n        x = self.msa(x) + x # skip connection #1\n        x = self.mlp(x) + x # skip connection #2\n        \n        return x\n    \n# ViT class\nclass ViT(nn.Module):\n    def __init__(self,\n                 img_size: int=224,\n                 in_channels: int=3,\n                 patch_size: int=16,\n                 layers: int=12,\n                 embedding_dim: int=192,\n                 mlp_size: int=768,\n                 num_heads: int=3,\n                 msa_dropout: float=0.0,\n                 mlp_dropout: float=0.1,\n                 emb_dropout: float=0.1,\n                 num_classes: int=1000):\n        super().__init__()\n        # calculating number of patches\n        self.num_patches = (img_size*img_size) // patch_size**2\n        # creating additional class embedding (token)\n        self.class_embedding = nn.Parameter(data=torch.rand(1, 1, embedding_dim), requires_grad=True)\n        # creating learnable positional embeddings\n        self.pos_embedding = nn.Parameter(data=torch.rand(1, self.num_patches+1, embedding_dim), requires_grad=True)\n        \n        self.emb_dropout = nn.Dropout(p=emb_dropout)\n        # image -> embeddings\n        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n                                              patch_size=patch_size,\n                                              embedding_dim=embedding_dim)\n        # transformer layers\n        self.encoder = nn.Sequential(*[\n            EncoderBlock(embedding_dim=embedding_dim,\n                         num_heads=num_heads,\n                         mlp_size=mlp_size,\n                         mlp_dropout=mlp_dropout,\n                         msa_dropout=msa_dropout)\n            for _ in range(layers)\n        ])\n        # classification head \n        self.head = nn.Sequential(\n            nn.LayerNorm(normalized_shape=embedding_dim),\n            nn.Linear(in_features=embedding_dim,\n                      out_features=num_classes)\n        )\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        # class token\n        cls_token = self.class_embedding.expand(batch_size, -1, -1) \n        \n        x = self.patch_embedding(x) # embeddings\n        x = torch.cat((cls_token, x), dim=1) # embeddings + class token\n        x = self.pos_embedding + x # making positional embeddings\n        x = self.emb_dropout(x)\n        x = self.encoder(x) # transformer layers\n        x = self.head(x[:, 0]) # classification is done based on class embedding (token)\n        \n        return x\n        \n# ViT_Ti16 configuration                    # compared to original ViT-B it has smaller D size and MLP size\nclass ViT_Ti_16(ViT):                       # as well as reduced number of heads \n    def __init__(self,                      # having less than 6 mln params. while ViT-B has 86 mln\n                 img_size: int=224,\n                 in_channels: int=3,\n                 patch_size: int=16,\n                 layers: int=12,\n                 embedding_dim: int=192,\n                 mlp_size: int=768,\n                 num_heads: int=3,\n                 msa_dropout: float=0.0,\n                 mlp_dropout: float=0.1,\n                 emb_dropout: float=0.1,\n                 num_classes: int=10):\n        super().__init__(patch_size=patch_size,\n                         layers=layers,\n                         num_classes=num_classes)\n \n\n#ViT_Ti32 configuration   \nclass ViT_Ti_32(ViT):\n    def __init__(self,\n                 img_size: int=224,\n                 in_channels: int=3,\n                 patch_size: int=32,\n                 layers: int=12,\n                 embedding_dim: int=192,\n                 mlp_size: int=768,\n                 num_heads: int=3,\n                 msa_dropout: float=0.0,\n                 mlp_dropout: float=0.1,\n                 emb_dropout: float=0.1,\n                 num_classes: int=10):\n        super().__init__(patch_size=patch_size,\n                         layers=layers,\n                         num_classes=num_classes)\n        \n   \n\n# ViT_Lite16 configuration   \nclass ViT_Lite_16(ViT):\n    def __init__(self,\n                 img_size: int=224,\n                 in_channels: int=3,\n                 patch_size: int=16,\n                 layers: int=7,\n                 embedding_dim: int=256,\n                 mlp_size: int=512,\n                 num_heads: int=4,\n                 msa_dropout: float=0.0,\n                 mlp_dropout: float=0.1,\n                 emb_dropout: float=0.1,\n                 num_classes: int=10):\n        super().__init__(patch_size=patch_size,\n                         layers=layers,\n                         num_classes=num_classes)\n     \n\n# ViT_Lite32 configuration   \nclass ViT_Lite_32(ViT):\n    def __init__(self,\n                 img_size: int=224,\n                 in_channels: int=3,\n                 patch_size: int=32,\n                 layers: int=7,\n                 embedding_dim: int=256,\n                 mlp_size: int=512,\n                 num_heads: int=4,\n                 msa_dropout: float=0.0,\n                 mlp_dropout: float=0.1,\n                 emb_dropout: float=0.1,\n                 num_classes: int=10):\n        super().__init__(patch_size=patch_size,\n                         layers=layers,\n                         num_classes=num_classes)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:03:00.879157Z","iopub.execute_input":"2023-12-03T11:03:00.879496Z","iopub.status.idle":"2023-12-03T11:03:00.911814Z","shell.execute_reply.started":"2023-12-03T11:03:00.879471Z","shell.execute_reply":"2023-12-03T11:03:00.910806Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# MultiHeadAttention Module\nclass MSA(nn.Module):\n    def __init__(self, embedding_dim: int=192, num_heads: int=3, dropout: float=0):\n        super().__init__()\n        # layer normalization layer\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n        # multiheadattention layer\n        self.msa_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n                                                   num_heads=num_heads,\n                                                   dropout=dropout,\n                                                   batch_first=True)\n        \n    def forward(self, x):\n        x = self.layer_norm(x)\n        # query, key, value are formed from the same x\n        x, _ = self.msa_attention(query=x,\n                                    key=x,\n                                    value=x,\n                                    need_weights=False)\n        return x\n    \n# MLP layer   \nclass MLP(nn.Module):\n    def __init__(self, \n                 embedding_dim: int=192,\n                 mlp_size: int=768,\n                 dropout: float=0.1):\n        super().__init__()\n        # layer normalization\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n        # two linear layers separated by GeLU activation\n        self.mlp = nn.Sequential(\n                nn.Linear(in_features=embedding_dim,\n                        out_features=mlp_size), # from input_dim to mlp_size\n                nn.GELU(),\n                nn.Dropout(p=dropout),\n                nn.Linear(in_features=mlp_size, # from mlp_size to input_dim\n                        out_features=embedding_dim),\n                nn.Dropout(p=dropout))\n        \n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n        \n        return x\n        \n# Transformer encoder block\nclass EncoderBlock(nn.Module):\n    def __init__(self,\n                 embedding_dim: int=192,\n                 num_heads: int=3,\n                 mlp_size: int=768,\n                 mlp_dropout: float=0.1,\n                 msa_dropout: float=0.0):\n        super().__init__()\n        # each block (layer) consists of MSA and MLP\n        self.msa = MSA(embedding_dim=embedding_dim,\n                       num_heads=num_heads,\n                       dropout=msa_dropout)\n        \n        self.mlp = MLP(embedding_dim=embedding_dim,\n                       mlp_size=mlp_size,\n                       dropout=mlp_dropout)\n        \n    def forward(self, x):\n        x = self.msa(x) + x # skip connection #1\n        x = self.mlp(x) + x # skip connection #2\n        \n        return x\n\n\n# taken from CVT-CCT paper\nclass Tokenizer(nn.Module):\n    def __init__(self,\n                 kernel_size: int=7, stride: int=2, padding: int=3,\n                 pooling_kernel_size: int=3, pooling_stride: int=2, pooling_padding: int=1,\n                 conv_layers: int=2, input_channels: int=3, \n                 output_channels: int=64, in_planes: int=64):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(input_channels, in_planes, \n                               kernel_size=(kernel_size, kernel_size),\n                               stride=(stride, stride),\n                               padding=(padding, padding), bias=False)\n        \n        self.max_pool = nn.MaxPool2d(kernel_size=pooling_kernel_size,\n                                     stride=pooling_stride,\n                                     padding=pooling_padding)\n        \n        self.conv2 = nn.Conv2d(in_planes, in_planes+in_planes,       # TODO change\n                               kernel_size=(kernel_size, kernel_size),\n                               stride=(stride, stride),\n                               padding=(padding, padding), bias=False)\n        \n        self.conv3 = nn.Conv2d(in_planes+in_planes, output_channels,\n                               kernel_size=(kernel_size, kernel_size),\n                               stride=(stride, stride),\n                               padding=(padding, padding), bias=False)\n\n        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.max_pool(x)\n        x = self.conv2(x)\n        x = self.max_pool(x)\n        x = self.conv3(x)\n        x = self.flatten(x)\n        \n        return x.permute(0, 2, 1)\n\n\nclass CvT(nn.Module):\n    def __init__(self,\n                 img_size: int=224, in_channels: int=3, kernel_size: int=7,\n                 layers: int=7, embedding_dim: int=256, mlp_size: int=512,\n                 num_heads: int=4, stride: int=2, padding: int=3,\n                 pooling_kernel_size: int=3, pooling_stride: int=2, pooling_padding :int=1,\n                 conv_layers: int=2, in_planes: int=64,\n                 msa_dropout: float=0.0, mlp_dropout: float=0.1,\n                 emb_dropout: float=0.1, num_classes: int=10):\n        super().__init__()\n        \n        self.tokenizer = Tokenizer(kernel_size=kernel_size, stride=stride, \n                                   padding=padding, pooling_kernel_size=pooling_kernel_size, \n                                   pooling_stride=pooling_stride, pooling_padding=pooling_padding, \n                                   conv_layers=conv_layers, input_channels=in_channels, \n                                   output_channels=embedding_dim, in_planes=in_planes)\n        \n        self.attention_pool = nn.Linear(embedding_dim, 1)\n        \n        self.emb_dropout = nn.Dropout(p=emb_dropout)\n        \n        self.encoder = nn.Sequential(*[\n            EncoderBlock(embedding_dim=embedding_dim,\n                         num_heads=num_heads,\n                         mlp_size=mlp_size,\n                         mlp_dropout=mlp_dropout,\n                         msa_dropout=msa_dropout)\n            for _ in range(layers)\n        ])\n        \n        self.norm = nn.LayerNorm(embedding_dim)\n        \n        self.head = nn.Sequential(\n            nn.LayerNorm(normalized_shape=embedding_dim),\n            nn.Linear(in_features=embedding_dim,\n                      out_features=num_classes)\n        )\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        \n        x = self.tokenizer(x)\n        x = self.emb_dropout(x)\n        x = self.encoder(x)\n        x = self.norm(x)\n        x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n        x = self.head(x)\n        \n        return x\n    \nclass CvT_3(CvT):\n    def __init__(self,\n                 img_size: int=224, in_channels: int=3, kernel_size: int=7,\n                 layers: int=3, embedding_dim: int=192, mlp_size: int=384,\n                 num_heads: int=3, stride: int=2, padding: int=3,\n                 pooling_kernel_size: int=3, pooling_stride: int=2, pooling_padding :int=1,\n                 conv_layers: int=2, in_planes: int=64,\n                 msa_dropout: float=0.0, mlp_dropout: float=0.1,\n                 emb_dropout: float=0.1, num_classes: int=10):\n        super().__init__(layers=layers, embedding_dim=embedding_dim, \n                         mlp_size=mlp_size, num_heads=num_heads)\n    \nclass CvT_7(CvT):\n    def __init__(self,\n                 img_size: int=224, in_channels: int=3, kernel_size: int=7,\n                 layers: int=7, embedding_dim: int=256, mlp_size: int=512,\n                 num_heads: int=4, stride: int=2, padding: int=3,\n                 pooling_kernel_size: int=3, pooling_stride: int=2, pooling_padding :int=1,\n                 conv_layers: int=2, in_planes: int=64,\n                 msa_dropout: float=0.0, mlp_dropout: float=0.1,\n                 emb_dropout: float=0.1, num_classes: int=10):\n        super().__init__(layers=layers, embedding_dim=embedding_dim, \n                         mlp_size=mlp_size, num_heads=num_heads)\n    \nclass CvT_14(CvT):\n    def __init__(self,\n                 img_size: int=224, in_channels: int=3, kernel_size: int=7,\n                 layers: int=14, embedding_dim: int=384, mlp_size: int=1152,\n                 num_heads: int=6, stride: int=2, padding: int=3,\n                 pooling_kernel_size: int=3, pooling_stride: int=2, pooling_padding :int=1,\n                 conv_layers: int=2, in_planes: int=64,\n                 msa_dropout: float=0.0, mlp_dropout: float=0.1,\n                 emb_dropout: float=0.1, num_classes: int=10):\n        super().__init__(layers=layers, embedding_dim=embedding_dim, \n                         mlp_size=mlp_size, num_heads=num_heads)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:10:51.373413Z","iopub.execute_input":"2023-12-03T11:10:51.373802Z","iopub.status.idle":"2023-12-03T11:10:51.410778Z","shell.execute_reply.started":"2023-12-03T11:10:51.373772Z","shell.execute_reply":"2023-12-03T11:10:51.409863Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"val_transforms = transforms.Compose([\n    transforms.Resize((config['IMG_SIZE'], config['IMG_SIZE'])),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                         std=[0.229, 0.224, 0.225]),\n])\n\nclass InferenceDataset(Dataset):\n    def __init__(self, images_dir, transform=None):\n        super().__init__()\n        self.images_dir = images_dir\n        self.transform = transform\n\n        self.images = os.listdir(images_dir)\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        img_name = self.images[index]\n        img_path = os.path.join(self.images_dir, img_name)\n\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n\n        return img_name, image","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:03:01.652241Z","iopub.execute_input":"2023-12-03T11:03:01.652986Z","iopub.status.idle":"2023-12-03T11:03:01.661064Z","shell.execute_reply.started":"2023-12-03T11:03:01.652954Z","shell.execute_reply":"2023-12-03T11:03:01.660158Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def inference(test_loader, model, config):    \n    model = model.to(config['DEVICE'])\n    model.eval()\n    \n    with torch.no_grad():\n        for data in tqdm(test_loader, desc='Inference'):\n            img_name, imgs = data\n            imgs = imgs.to(config['DEVICE'])\n\n            start_time = time.time()\n            logits = model(imgs)\n            \n            total_time = time.time() - start_time\n            probas = F.softmax(logits, dim=1)\n            \n            print(f\"Time elapsed: {total_time} sec\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:03:02.076472Z","iopub.execute_input":"2023-12-03T11:03:02.077288Z","iopub.status.idle":"2023-12-03T11:03:02.083422Z","shell.execute_reply.started":"2023-12-03T11:03:02.077256Z","shell.execute_reply":"2023-12-03T11:03:02.082453Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"test_transformations = val_transforms\ntest_dataset = InferenceDataset(config['TEST_DIR'], transform=test_transformations)\nlength = len(test_dataset)\nindices = torch.randperm(length)[:int(length*0.00314)]\ntest_subset = Subset(test_dataset, indices)\ntest_loader = DataLoader(test_subset, batch_size=config['BATCH_SIZE'], shuffle=False, num_workers=config['NUM_WORKERS'])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:03:02.803629Z","iopub.execute_input":"2023-12-03T11:03:02.804006Z","iopub.status.idle":"2023-12-03T11:03:03.521709Z","shell.execute_reply.started":"2023-12-03T11:03:02.803974Z","shell.execute_reply":"2023-12-03T11:03:03.520631Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## ResNets","metadata":{}},{"cell_type":"code","source":"model = ResNet50()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:03:03.709295Z","iopub.execute_input":"2023-12-03T11:03:03.709626Z","iopub.status.idle":"2023-12-03T11:03:03.946302Z","shell.execute_reply.started":"2023-12-03T11:03:03.709598Z","shell.execute_reply":"2023-12-03T11:03:03.945507Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"inference(test_loader, model, config)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:03:43.755814Z","iopub.execute_input":"2023-12-03T11:03:43.756747Z","iopub.status.idle":"2023-12-03T11:03:45.635145Z","shell.execute_reply.started":"2023-12-03T11:03:43.756710Z","shell.execute_reply":"2023-12-03T11:03:45.634128Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Inference: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it]","output_type":"stream"},{"name":"stdout","text":"Time elapsed: 0.012400150299072266 sec\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"model = ResNet101()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:04:24.405794Z","iopub.execute_input":"2023-12-03T11:04:24.406664Z","iopub.status.idle":"2023-12-03T11:04:24.795441Z","shell.execute_reply.started":"2023-12-03T11:04:24.406631Z","shell.execute_reply":"2023-12-03T11:04:24.794663Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"inference(test_loader, model, config)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:04:33.670074Z","iopub.execute_input":"2023-12-03T11:04:33.670858Z","iopub.status.idle":"2023-12-03T11:04:35.607441Z","shell.execute_reply.started":"2023-12-03T11:04:33.670816Z","shell.execute_reply":"2023-12-03T11:04:35.606233Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Inference: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]","output_type":"stream"},{"name":"stdout","text":"Time elapsed: 0.021353721618652344 sec\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## ViT","metadata":{}},{"cell_type":"code","source":"model = ViT_Lite_16()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:17:51.887712Z","iopub.execute_input":"2023-12-03T11:17:51.888065Z","iopub.status.idle":"2023-12-03T11:17:51.928772Z","shell.execute_reply.started":"2023-12-03T11:17:51.888041Z","shell.execute_reply":"2023-12-03T11:17:51.927876Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"inference(test_loader, model, config)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:18:04.054848Z","iopub.execute_input":"2023-12-03T11:18:04.055782Z","iopub.status.idle":"2023-12-03T11:18:05.882587Z","shell.execute_reply.started":"2023-12-03T11:18:04.055745Z","shell.execute_reply":"2023-12-03T11:18:05.881530Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stderr","text":"Inference: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Time elapsed: 0.006203174591064453 sec\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"model = ViT_Lite_32()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:18:22.937398Z","iopub.execute_input":"2023-12-03T11:18:22.937808Z","iopub.status.idle":"2023-12-03T11:18:22.979861Z","shell.execute_reply.started":"2023-12-03T11:18:22.937775Z","shell.execute_reply":"2023-12-03T11:18:22.979091Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"inference(test_loader, model, config)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:18:30.109889Z","iopub.execute_input":"2023-12-03T11:18:30.110760Z","iopub.status.idle":"2023-12-03T11:18:31.891981Z","shell.execute_reply.started":"2023-12-03T11:18:30.110730Z","shell.execute_reply":"2023-12-03T11:18:31.890867Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stderr","text":"Inference: 100%|██████████| 1/1 [00:01<00:00,  1.74s/it]","output_type":"stream"},{"name":"stdout","text":"Time elapsed: 0.005919218063354492 sec\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## CCT with 2 convolutional layers","metadata":{}},{"cell_type":"code","source":"model = CvT_3()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:07:56.404832Z","iopub.execute_input":"2023-12-03T11:07:56.405793Z","iopub.status.idle":"2023-12-03T11:07:56.427908Z","shell.execute_reply.started":"2023-12-03T11:07:56.405756Z","shell.execute_reply":"2023-12-03T11:07:56.427145Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"inference(test_loader, model, config)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:08:04.932236Z","iopub.execute_input":"2023-12-03T11:08:04.933017Z","iopub.status.idle":"2023-12-03T11:08:06.782521Z","shell.execute_reply.started":"2023-12-03T11:08:04.932980Z","shell.execute_reply":"2023-12-03T11:08:06.781398Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"Inference: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]","output_type":"stream"},{"name":"stdout","text":"Time elapsed: 0.004708290100097656 sec\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"model = CvT_7()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:08:49.371613Z","iopub.execute_input":"2023-12-03T11:08:49.372401Z","iopub.status.idle":"2023-12-03T11:08:49.419593Z","shell.execute_reply.started":"2023-12-03T11:08:49.372370Z","shell.execute_reply":"2023-12-03T11:08:49.418848Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"inference(test_loader, model, config)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:08:58.670006Z","iopub.execute_input":"2023-12-03T11:08:58.670653Z","iopub.status.idle":"2023-12-03T11:09:00.496391Z","shell.execute_reply.started":"2023-12-03T11:08:58.670619Z","shell.execute_reply":"2023-12-03T11:09:00.495227Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"Inference: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Time elapsed: 0.006447315216064453 sec\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"model = CvT_14()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:09:24.794733Z","iopub.execute_input":"2023-12-03T11:09:24.795497Z","iopub.status.idle":"2023-12-03T11:09:24.979932Z","shell.execute_reply.started":"2023-12-03T11:09:24.795464Z","shell.execute_reply":"2023-12-03T11:09:24.979145Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"inference(test_loader, model, config)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:09:36.482785Z","iopub.execute_input":"2023-12-03T11:09:36.483188Z","iopub.status.idle":"2023-12-03T11:09:38.332878Z","shell.execute_reply.started":"2023-12-03T11:09:36.483156Z","shell.execute_reply":"2023-12-03T11:09:38.331751Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"Inference: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]","output_type":"stream"},{"name":"stdout","text":"Time elapsed: 0.009548187255859375 sec\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## CCT with 3 convolutional layers","metadata":{}},{"cell_type":"code","source":"model = CvT_3()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:11:00.476649Z","iopub.execute_input":"2023-12-03T11:11:00.477533Z","iopub.status.idle":"2023-12-03T11:11:00.506432Z","shell.execute_reply.started":"2023-12-03T11:11:00.477500Z","shell.execute_reply":"2023-12-03T11:11:00.505650Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"inference(test_loader, model, config)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:11:09.631806Z","iopub.execute_input":"2023-12-03T11:11:09.632556Z","iopub.status.idle":"2023-12-03T11:11:11.508229Z","shell.execute_reply.started":"2023-12-03T11:11:09.632525Z","shell.execute_reply":"2023-12-03T11:11:11.507126Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"Inference: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it]","output_type":"stream"},{"name":"stdout","text":"Time elapsed: 0.0048029422760009766 sec\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"model = CvT_7()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:11:32.095318Z","iopub.execute_input":"2023-12-03T11:11:32.095711Z","iopub.status.idle":"2023-12-03T11:11:32.152299Z","shell.execute_reply.started":"2023-12-03T11:11:32.095681Z","shell.execute_reply":"2023-12-03T11:11:32.151399Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"inference(test_loader, model, config)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:11:41.459142Z","iopub.execute_input":"2023-12-03T11:11:41.459550Z","iopub.status.idle":"2023-12-03T11:11:43.320449Z","shell.execute_reply.started":"2023-12-03T11:11:41.459521Z","shell.execute_reply":"2023-12-03T11:11:43.319369Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stderr","text":"Inference: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]","output_type":"stream"},{"name":"stdout","text":"Time elapsed: 0.0067441463470458984 sec\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"model = CvT_14()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:12:02.576955Z","iopub.execute_input":"2023-12-03T11:12:02.577906Z","iopub.status.idle":"2023-12-03T11:12:02.772420Z","shell.execute_reply.started":"2023-12-03T11:12:02.577869Z","shell.execute_reply":"2023-12-03T11:12:02.771558Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"inference(test_loader, model, config)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:12:12.935840Z","iopub.execute_input":"2023-12-03T11:12:12.936615Z","iopub.status.idle":"2023-12-03T11:12:14.804799Z","shell.execute_reply.started":"2023-12-03T11:12:12.936581Z","shell.execute_reply":"2023-12-03T11:12:14.803685Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stderr","text":"Inference: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]","output_type":"stream"},{"name":"stdout","text":"Time elapsed: 0.010434865951538086 sec\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}